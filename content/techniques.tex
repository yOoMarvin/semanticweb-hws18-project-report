\section{Techniques}
One of the main ideas during development was the decoupling of major steps in Semap’s processing pipeline. So the project is split into three smaller part: The user interface, the processing pipeline and REST interfaces. 
The frontend website is primarily responsible for the rendering of visual elements. The computation of clusters takes place on a separate backend server, which is contacted by the website using a REST-interface on the same backend server. The following sections will be based on this trisection and elaborate in more detail on each of the components.

\subsection{User Interface}


\subsection{REST Interface}
When a request is sent to the REST interface, the server reads given parameters and schedules the crawling and clustering of the data as well as the assembly of a response in the GeoJSON format. So the interface does not execute any functional computations; it rather passes on the request to different execution modules. The REST interface relies on the JAX-RS implementation, also known as Jersey with Java Servlet Annotations 3.0. On a server startup, a glassfish servlet container is created inside any webserver and requests to the webserver forwarded to the container. The container itself requires little configuration, as most of it is done by coding and Java 3.0 annotations. In the container, a usual servlet-container runs, which passes the requests to other components as described in the following sections. 

\subsection{Processing Pipeline}
A core object structure underlies the processing pipeline, which is utilized whenever the backend server is contacted by the UI website. I.e. After crawling the data from respective servers, the results are saved in internal object representations and read when the processing takes place. The core object structure is tuned towards the task of semantically clustering regions and thus facilitates the use of multiple clustering implementations with diverse libraries. The UML class diagram in figure \ref{fig:cos} summarizes the object structure:
 
\begin{figure}
  \centering
    \includegraphics[scale=0.8]{./content/cos.png}
  \caption{UML class diagram of the core object structure}\label{fig:cos}
\end{figure}


The figure shows the centrality of the Instance class. All other attributes interact with it, as they rely on the information held in instance objects. Practically speaking, an instance represents a real-world institution like a specific bar or school. Each object of the instance class can have a number of Categories assigned to it. Categories make up the semantic context of an institution. They have a name like “museum” or “bar” and a weight. The semap project assumes, that the deeper a category is in its ontology, the more informative it is. That is why a detailed category like “Irish pub” is meant to get a higher weight than higher-level categories like “thing”. The clustering algorithm can then prefer to place instances based on the deep-level knowledge into clusters, resulting in more specific clusters.
Before clustering takes place, no instance is assigned to a cluster, as no cluster exists. Ideally after clustering each instance is assigned to exactly one cluster. When running a clustering algorithm (here represented by an abstract class), it will create new objects of type Cluster using a list of instances and assign those instances to a cluster. The different implementations of clustering algorithms are not depicted in the figure for abstraction purposes. But in the end, the actual implementations are of type ClusteringAlgorithm, so they just inherit from the abstract class ClusteringAlgorithm and the interface IClusteringAlgorithm. In the project, a simple square clustering algorithm and a DBScan were implemented.


\paragraph{Simple Square Clustering }
The simple square clustering algorithm a rather straight-forward implementation of clustering algorithms. It considers each element from the list of instances and creates clusters based on their position. So for each instance, the simple square clustering implementation will check if there is already a cluster, in which the current instance is located and will assign it to this cluster or create a new cluster correspondingly. After having done so for the entire list of instances, each clusters gets a category. The computation is based on a scoring function, which determines the times, a category appears in a cluster and weights each appearance by the category’s weight-property. The category with the highest weight is then allocated to a cluster. Algorithm \ref{alg:ssca} formalizes the procedures again. In sum, the simple square clustering algorithm does not take into account the semantic context (i.e. the categories) of instances and only uses positional information initially. That is why the results are not guaranteed to be semantically meaningful. However, the algorithm runs very fast, as it is linked to very little computational overhead.


\begin{algorithm}
\caption{Sketch of the simple quare clustering algorithm}\label{alg:ssca}
\begin{algorithmic} 
\STATE $I \gets \text{set~of~all~instances}$
\STATE $C \gets \emptyset$
\FORALL{$i \in I$}
\FORALL{$c \in C$}
 \IF{$i~located~in~c$}
  \STATE $c.add(i)$
  \ELSE 
  \STATE $x \gets createCluster(i.latitude, i.longitude)$
  \STATE $C \gets C \cap x$
\ENDIF
\ENDFOR
\ENDFOR
\STATE ${CAT} \gets \emptyset$
\FORALL{$i \in C.getInstances()$}
\FORALL{$category \in i.getCategories()$}
\STATE ${CAT}_{category} \gets {CAT}_{category}.getScore() + category.getScore()$
\STATE ${CAT} \gets {CAT} \cap {CAT}_{category}$
\ENDFOR
\ENDFOR
\RETURN $max({CAT}.getAllScores())$
\end{algorithmic}
\end{algorithm}

\paragraph{DBScan}
DBScan is a the sophisticated clustering algorithm in the semap project and relies on the Weka library implementation. The DBScan tries to build clusters based on two pieces of information: The position of instances and their categories. The position is determined by the longitude and latitude coordinates and is normalized on a scale from zero to one; alternatively it can be z-normalized by a parameter setting in the Java coding. Categories are one-hot-encoded, so that each instance gets a vectors consisting of values in the range from one to zero. The vector represents the set of all categories. Each category is represented by a positional value in the vector. All the categories an instance does not belong to, take the value 0 in the vector. The categories, an instance belongs to, take a normalized value greater than zero. The value is determined by the formula: 
\begin{equation}
n := number~of~categories~of~an~instance
\end{equation}
\begin{equation}
w_i := weight~of~the~i\-th~category~of~an~instance
\end{equation}
\begin{equation}
w_c := weight~of~current~category
\end{equation}
\begin{equation}\label{eq:normalization}
val = \frac{w_c}{\sum_{i=0}^{n}w_i}
\end{equation}


Weight of current category/sum of categories of an instance(weight)
So the value depends on the number of instance’s categories and the weight of the respective categories. This method ensures, that the weighted vector values have the same impact on the clustering as the positional information latitude and longitude. Before the DBScan algorithm can start working in Weka, a preprocessing pipeline transform the internal object representation as described in figure \ref{fig:cos} into the object representation needed by the Weka library. It therefore writes an artificial .arff-file to disk and reads it again with tools from the weka library. Afterwards, the DBScan is executed and the results again returned into the core object representation of semap. The entire procedure of the DBScan in the semap project is summarized in algorithm \ref{alg:dbscan} as follows:

\begin{algorithm}
\caption{Sketch of the DBScan clustering algorithm}\label{alg:dbscan}
\begin{algorithmic} 
\STATE $I \gets \text{set~of~all~instances}$
\STATE $C \gets \emptyset$
\FORALL{$i \in I$}
\STATE $x \gets normalize(i.latitude)$
\STATE $i.setLatitude(x)$
\STATE $x \gets normalize(i.longitude)$
\STATE $i.setLongitude(x)$
\STATE $\vec{y} \gets oneHotEncode(i.categories)$
\FORALL{$z \in \vec{y}$}
\STATE $z \gets normalize~z~as~to~formula~(\ref{eq:normalization})$
\ENDFOR
\STATE $i.categories \gets \vec{y}$
\ENDFOR
\STATE $dBScan(I)$
\STATE ${CAT} \gets \emptyset$
\FORALL{$i \in C.getInstances()$}
\FORALL{$category \in i.getCategories()$}
\STATE ${CAT}_{category} \gets {CAT}_{category}.getScore() + category.getScore()$
\STATE ${CAT} \gets {CAT} \cap {CAT}_{category}$
\ENDFOR
\ENDFOR
\RETURN $max({CAT}.getAllScores())$
\end{algorithmic}
\end{algorithm}

In sum, the value normalization and object transformations are computationally expensive operations. While for example the simple square algorithm clusters multiple millions of instances within few milliseconds, the DBScan requires about 30 seconds to do so with only 15 thousand instances. However, the major part of this timeframe is not spent in the normalization and object transformation implementation. The most time is actually needed by Weka’s DBScan algorithm itself. So this runtime behavior is a major limitation of the DBScan algorithm. Nonetheless, the DBScan algorithm is capable of finding much more meaningful results. It considers categories of instances already when performing the clustering and not afterwards. It is also not limited to geographically speaking squared clusters, but has arbitrary decision boundaries. That makes this algorithm a very powerful tool for detailed analyses on regional, public data.

